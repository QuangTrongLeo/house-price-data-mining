{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b203ee8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1460, 81)\n",
      "Columns: 81\n",
      "Train: 1285 rows (88.0%)\n",
      "Test:  175 rows (12.0%)\n",
      "\n",
      "Train years: 2006 - 2009\n",
      "Test years:  2010 - 2010\n",
      "basic_impute() defined\n",
      "quality_map defined\n",
      "{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, nan: 0}\n",
      "Feature groups loaded:\n",
      " Location & Lot:     14\n",
      " Size & Interior:    14\n",
      " Quality & Condition:14\n",
      " Amenities/Sale:     9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import category_encoders as ce\n",
    "from prophet import Prophet\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "\n",
    "# Train/Test split theo nƒÉm\n",
    "df_train = df[df['YrSold'] < 2010].copy()\n",
    "df_test  = df[df['YrSold'] >= 2010].copy()\n",
    "\n",
    "print(f\"Train: {len(df_train)} rows ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(df_test)} rows ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain years: {df_train['YrSold'].min()} - {df_train['YrSold'].max()}\")\n",
    "print(f\"Test years:  {df_test['YrSold'].min()} - {df_test['YrSold'].max()}\")\n",
    "\n",
    "\n",
    "def basic_impute(df_in):\n",
    "    \"\"\"Impute missing: median for numeric, mode for categorical.\"\"\"\n",
    "    df = df_in.copy()\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c].fillna(df[c].median(), inplace=True)\n",
    "\n",
    "    for c in cat_cols:\n",
    "        if df[c].isna().any():\n",
    "            mode_val = df[c].mode().iloc[0] if not df[c].mode().empty else \"NA\"\n",
    "            df[c].fillna(mode_val, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"basic_impute() defined\")\n",
    "\n",
    "quality_map = {\n",
    "    \"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, np.nan: 0\n",
    "}\n",
    "\n",
    "print(\"quality_map defined\")\n",
    "print(quality_map)\n",
    "\n",
    "\n",
    "location_lot_cols = [\n",
    "    \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "    \"Street\", \"Alley\", \"LotShape\", \"LandContour\",\n",
    "    \"Utilities\", \"LotConfig\", \"LandSlope\",\n",
    "    \"Neighborhood\", \"Condition1\", \"Condition2\"\n",
    "]\n",
    "\n",
    "size_interior_cols = [\n",
    "    \"GrLivArea\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n",
    "    \"TotRmsAbvGrd\", \"BedroomAbvGr\", \"KitchenAbvGr\",\n",
    "    \"GarageArea\", \"GarageCars\", \"MasVnrArea\"\n",
    "]\n",
    "\n",
    "quality_condition_cols = [\n",
    "    \"OverallQual\", \"OverallCond\", \"YearBuilt\", \"YearRemodAdd\",\n",
    "    \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\",\n",
    "    \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\",\n",
    "    \"GarageQual\", \"GarageCond\", \"PoolQC\"\n",
    "]\n",
    "\n",
    "amenities_sale_time_cols = [\n",
    "    \"Fireplaces\", \"GarageYrBlt\", \"PoolArea\", \"Fence\",\n",
    "    \"MiscVal\", \"MoSold\", \"YrSold\", \"SaleType\", \"SaleCondition\"\n",
    "]\n",
    "\n",
    "groups_info = {\n",
    "    \"Location & Lot\": location_lot_cols,\n",
    "    \"Size & Interior\": size_interior_cols,\n",
    "    \"Quality & Condition\": quality_condition_cols,\n",
    "    \"Amenities, Sale & Time\": amenities_sale_time_cols\n",
    "}\n",
    "\n",
    "# Ch·ªâ gi·ªØ c·ªôt t·ªìn t·∫°i trong dataset\n",
    "location_lot_cols        = [c for c in location_lot_cols if c in df.columns]\n",
    "size_interior_cols       = [c for c in size_interior_cols if c in df.columns]\n",
    "quality_condition_cols   = [c for c in quality_condition_cols if c in df.columns]\n",
    "amenities_sale_time_cols = [c for c in amenities_sale_time_cols if c in df.columns]\n",
    "\n",
    "print(\"Feature groups loaded:\")\n",
    "print(f\" Location & Lot:     {len(location_lot_cols)}\")\n",
    "print(f\" Size & Interior:    {len(size_interior_cols)}\")\n",
    "print(f\" Quality & Condition:{len(quality_condition_cols)}\")\n",
    "print(f\" Amenities/Sale:     {len(amenities_sale_time_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4587af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy1_onehot_standard() defined\n",
      "\n",
      "  Strategy 1 Results:\n",
      "   Train shape: (1285, 287)\n",
      "   Test shape: (175, 287)\n",
      "   Total features: 286\n",
      "\n",
      "Sample features:\n",
      "['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1']\n",
      "Train and test have matching columns\n",
      "\n",
      "Train Statistics:\n",
      "                 Id    MSSubClass   LotFrontage      LotArea   OverallQual\n",
      "count  1.285000e+03  1.285000e+03  1.285000e+03  1285.000000  1.285000e+03\n",
      "mean   1.769445e-16  7.326608e-17  1.009137e-16     0.000000  2.737110e-16\n",
      "std    1.000389e+00  1.000389e+00  1.000389e+00     1.000389  1.000389e+00\n",
      "min   -1.752369e+00 -8.799154e-01 -2.189551e+00    -0.892342 -3.676904e+00\n",
      "25%   -8.523952e-01 -8.799154e-01 -4.357896e-01    -0.294695 -8.003002e-01\n",
      "50%   -2.685988e-03 -1.642880e-01 -3.107533e-02    -0.102869 -8.114932e-02\n",
      "75%    8.685652e-01  3.127969e-01  4.186071e-01     0.104862  6.380015e-01\n",
      "max    1.739816e+00  3.175306e+00  1.094118e+01    19.730836  2.795454e+00\n",
      "strategy2_target_robust() defined\n",
      "\n",
      "  Strategy 2 Results:\n",
      "   Train shape: (1285, 81)\n",
      "   Test shape: (175, 81)\n",
      "   Total features: 80\n",
      "Train and test have matching columns\n",
      "\n",
      "Train Statistics:\n",
      "                Id   MSSubClass  LotFrontage      LotArea  OverallQual\n",
      "count  1285.000000  1285.000000  1285.000000  1285.000000  1285.000000\n",
      "mean      0.001561     0.137743     0.036371     0.257458     0.056420\n",
      "std       0.581297     0.838752     1.170872     2.503751     0.695535\n",
      "min      -1.016690    -0.600000    -2.526316    -1.975875    -2.500000\n",
      "25%      -0.493741    -0.600000    -0.473684    -0.480097    -0.500000\n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "75%       0.506259     0.400000     0.526316     0.519903     0.500000\n",
      "max       1.012517     2.800000    12.842105    49.639324     2.000000\n",
      "\n",
      "  Strategy Comparison:\n",
      "Strategy 1: 286 features\n",
      "Strategy 2: 80 features\n"
     ]
    }
   ],
   "source": [
    "def strategy1_onehot_standard(df_in, reference_columns=None):\n",
    "    \"\"\"One-Hot Encoding cho categorical + StandardScaler cho numeric\"\"\"\n",
    "    df = basic_impute(df_in)\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols and c != \"SalePrice\"]\n",
    "    \n",
    "    # One-hot encoding\n",
    "    df_cat = pd.get_dummies(df[cat_cols].astype(str), drop_first=False)\n",
    "    \n",
    "    # If reference columns provided, align to them\n",
    "    if reference_columns is not None:\n",
    "        cat_ref_cols = [c for c in reference_columns if c not in num_cols and c != \"SalePrice\"]\n",
    "        for col in cat_ref_cols:\n",
    "            if col not in df_cat.columns:\n",
    "                df_cat[col] = 0\n",
    "        df_cat = df_cat[cat_ref_cols]\n",
    "    \n",
    "    # Standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    df_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    result = pd.concat([df_num, df_cat], axis=1)\n",
    "    if \"SalePrice\" in df.columns:\n",
    "        result[\"SalePrice\"] = df[\"SalePrice\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"strategy1_onehot_standard() defined\")\n",
    "\n",
    "\n",
    "proc1_train = strategy1_onehot_standard(df_train)\n",
    "proc1_test = strategy1_onehot_standard(df_test, reference_columns=proc1_train.columns)\n",
    "\n",
    "print(\"\\n  Strategy 1 Results:\")\n",
    "print(f\"   Train shape: {proc1_train.shape}\")\n",
    "print(f\"   Test shape: {proc1_test.shape}\")\n",
    "print(f\"   Total features: {proc1_train.shape[1] - 1}\")\n",
    "print(f\"\\nSample features:\")\n",
    "print(proc1_train.columns[:10].tolist())\n",
    "\n",
    "# Verify columns match\n",
    "assert list(proc1_train.columns) == list(proc1_test.columns), \"Column mismatch!\"\n",
    "print(\"Train and test have matching columns\")\n",
    "\n",
    "\n",
    "print(\"\\nTrain Statistics:\")\n",
    "print(proc1_train.describe().iloc[:, :5])\n",
    "\n",
    "\n",
    "def strategy2_target_robust(df_in, target=\"SalePrice\", reference_columns=None):\n",
    "    \"\"\"Target Encoding cho categorical + RobustScaler cho numeric\"\"\"\n",
    "    if ce is None:\n",
    "        print(\"  category_encoders not available, skipping Strategy 2\")\n",
    "        return None\n",
    "    \n",
    "    df = basic_impute(df_in)\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols and c != target]\n",
    "    \n",
    "    # Target encoding\n",
    "    te = ce.TargetEncoder(cols=cat_cols, smoothing=0.3)\n",
    "    df_cat = te.fit_transform(\n",
    "        df[cat_cols],\n",
    "        df[target] if target in df.columns else None\n",
    "    )\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler = RobustScaler()\n",
    "    df_num = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    result = pd.concat([df_num, df_cat], axis=1)\n",
    "    \n",
    "    # Align columns if reference provided\n",
    "    if reference_columns is not None:\n",
    "        ref_cols = [c for c in reference_columns if c != target]\n",
    "        for col in ref_cols:\n",
    "            if col not in result.columns:\n",
    "                result[col] = 0\n",
    "        result = result[ref_cols]\n",
    "    \n",
    "    if target in df.columns:\n",
    "        result[target] = df[target]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"strategy2_target_robust() defined\")\n",
    "\n",
    "\n",
    "if ce is not None:\n",
    "    proc2_train = strategy2_target_robust(df_train)\n",
    "    proc2_test = strategy2_target_robust(df_test, reference_columns=proc2_train.columns)\n",
    "    \n",
    "    print(\"\\n  Strategy 2 Results:\")\n",
    "    print(f\"   Train shape: {proc2_train.shape}\")\n",
    "    print(f\"   Test shape: {proc2_test.shape}\")\n",
    "    print(f\"   Total features: {proc2_train.shape[1] - 1}\")\n",
    "    \n",
    "    # Verify columns match\n",
    "    assert list(proc2_train.columns) == list(proc2_test.columns), \"Column mismatch!\"\n",
    "    print(\"Train and test have matching columns\")\n",
    "    \n",
    "    print(\"\\nTrain Statistics:\")\n",
    "    print(proc2_train.describe().iloc[:, :5])\n",
    "else:\n",
    "    proc2_train = proc2_test = None\n",
    "    print(\"\\n  Strategy 2 skipped (category_encoders not available)\")\n",
    "\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\n  Strategy Comparison:\")\n",
    "print(f\"Strategy 1: {proc1_train.shape[1] - 1} features\")\n",
    "if proc2_train is not None:\n",
    "    print(f\"Strategy 2: {proc2_train.shape[1] - 1} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a198e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy3_pca_ordinal() ready\n",
      "[Strategy 3] PCA variance explained: 58.86%\n",
      "[Strategy 3] PCA variance explained: 60.05%\n",
      "\n",
      "Strategy 3 Results:\n",
      " Train shape: (1285, 37)\n",
      " Test shape:  (175, 37)\n",
      " Features: 36\n",
      " Columns match \n",
      "strategy4_minmax_embedding() ready\n",
      "\n",
      "Strategy 4 Results:\n",
      " Train shape: (1285, 235)\n",
      " Test shape:  (175, 235)\n",
      " Features: 234\n",
      " Columns match \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 3: PCA FOR SIZE + ORDINAL FOR QUALITY\n",
    "# ============================================================\n",
    "\n",
    "def strategy3_pca_ordinal(df_in, pca_components=3, reference_columns=None):\n",
    "    df = basic_impute(df_in)\n",
    "\n",
    "    # ==== 1. SIZE FEATURES PCA ====\n",
    "    size_present = [c for c in size_interior_cols if c in df.columns]\n",
    "\n",
    "    if size_present:\n",
    "        scaler_size = StandardScaler()\n",
    "        size_scaled = scaler_size.fit_transform(df[size_present])\n",
    "\n",
    "        n_components = min(pca_components, len(size_present))\n",
    "        pca = PCA(n_components=n_components)\n",
    "        size_pca = pca.fit_transform(size_scaled)\n",
    "\n",
    "        size_pca_df = pd.DataFrame(\n",
    "            size_pca,\n",
    "            columns=[f\"SizePCA{i+1}\" for i in range(n_components)],\n",
    "            index=df.index\n",
    "        )\n",
    "        print(f\"[Strategy 3] PCA variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    else:\n",
    "        size_pca_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # ==== 2. QUALITY FEATURES ORDINAL ====\n",
    "    quality_present = [c for c in quality_condition_cols if c in df.columns]\n",
    "\n",
    "    quality_df = pd.DataFrame(index=df.index)\n",
    "    for c in quality_present:\n",
    "        if df[c].dtype == object:\n",
    "            quality_df[c + \"_ord\"] = df[c].map(quality_map).fillna(0).astype(int)\n",
    "        else:\n",
    "            quality_df[c + \"_num\"] = df[c]\n",
    "\n",
    "    # ==== 3. OTHER NUMERIC ====\n",
    "    other_cols = [\n",
    "        c for c in df.select_dtypes(include=[np.number]).columns\n",
    "        if c not in size_present + quality_present + [\"SalePrice\"]\n",
    "    ]\n",
    "\n",
    "    if other_cols:\n",
    "        scaler_other = StandardScaler()\n",
    "        other_df = pd.DataFrame(\n",
    "            scaler_other.fit_transform(df[other_cols]),\n",
    "            columns=other_cols,\n",
    "            index=df.index\n",
    "        )\n",
    "    else:\n",
    "        other_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Combine all\n",
    "    result = pd.concat([size_pca_df, quality_df, other_df], axis=1)\n",
    "\n",
    "    # ==== ALIGN TEST SET ====\n",
    "    if reference_columns is not None:\n",
    "        ref_cols = [c for c in reference_columns if c != \"SalePrice\"]\n",
    "        for col in ref_cols:\n",
    "            if col not in result.columns:\n",
    "                result[col] = 0\n",
    "        result = result[ref_cols]\n",
    "\n",
    "    # Add target\n",
    "    if \"SalePrice\" in df.columns:\n",
    "        result[\"SalePrice\"] = df[\"SalePrice\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"strategy3_pca_ordinal() ready\")\n",
    "\n",
    "# Apply Strategy 3\n",
    "proc3_train = strategy3_pca_ordinal(df_train, pca_components=3)\n",
    "proc3_test  = strategy3_pca_ordinal(df_test, pca_components=3, reference_columns=proc3_train.columns)\n",
    "\n",
    "print(\"\\nStrategy 3 Results:\")\n",
    "print(f\" Train shape: {proc3_train.shape}\")\n",
    "print(f\" Test shape:  {proc3_test.shape}\")\n",
    "print(f\" Features: {proc3_train.shape[1] - 1}\")\n",
    "assert list(proc3_train.columns) == list(proc3_test.columns)\n",
    "print(\" Columns match \")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 4: MINMAX + EMBEDDING\n",
    "# ============================================================\n",
    "\n",
    "def strategy4_minmax_embedding(df_in, reference_columns=None):\n",
    "    df = basic_impute(df_in)\n",
    "\n",
    "    # === NUMERIC ===\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c != \"SalePrice\"]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    num_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(df[num_cols]),\n",
    "        columns=num_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # === CATEGORICAL ===\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    cat_df = pd.DataFrame(index=df.index)\n",
    "    emb_info = {}\n",
    "\n",
    "    for c in cat_cols:\n",
    "        nunique = df[c].nunique(dropna=False)\n",
    "\n",
    "        if nunique > 10:\n",
    "            # Integer encoding for embedding\n",
    "            codes, uniques = pd.factorize(df[c].astype(str))\n",
    "            cat_df[c + \"_idx\"] = codes\n",
    "            emb_info[c] = {\n",
    "                \"vocab_size\": len(uniques),\n",
    "                \"embedding_dim\": min(50, (len(uniques) + 1) // 2)\n",
    "            }\n",
    "        else:\n",
    "            # One-hot encoding for small cardinality\n",
    "            dummies = pd.get_dummies(df[c].astype(str), prefix=c, drop_first=False)\n",
    "            cat_df = pd.concat([cat_df, dummies], axis=1)\n",
    "\n",
    "    # Combine numeric + categorical\n",
    "    result = pd.concat([num_scaled, cat_df], axis=1)\n",
    "\n",
    "    # Align with train reference\n",
    "    if reference_columns is not None:\n",
    "        ref_cols = [c for c in reference_columns if c != \"SalePrice\"]\n",
    "        for col in ref_cols:\n",
    "            if col not in result.columns:\n",
    "                result[col] = 0\n",
    "        result = result[ref_cols]\n",
    "\n",
    "    # Add target\n",
    "    if \"SalePrice\" in df.columns:\n",
    "        result[\"SalePrice\"] = df[\"SalePrice\"]\n",
    "\n",
    "    return result, emb_info\n",
    "\n",
    "print(\"strategy4_minmax_embedding() ready\")\n",
    "\n",
    "# Apply Strategy 4\n",
    "proc4_train, emb_info = strategy4_minmax_embedding(df_train)\n",
    "proc4_test, _ = strategy4_minmax_embedding(df_test, reference_columns=proc4_train.columns)\n",
    "\n",
    "print(\"\\nStrategy 4 Results:\")\n",
    "print(f\" Train shape: {proc4_train.shape}\")\n",
    "print(f\" Test shape:  {proc4_test.shape}\")\n",
    "print(f\" Features: {proc4_train.shape[1] - 1}\")\n",
    "assert list(proc4_train.columns) == list(proc4_test.columns)\n",
    "print(\" Columns match \")\n",
    "\n",
    "# Sample em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078a6c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " compute_correlations() defined\n",
      "  Top 15 Features by Pearson Correlation:\n",
      "               pearson  spearman\n",
      "OverallQual   0.791936  0.806647\n",
      "GrLivArea     0.707788  0.732569\n",
      "GarageCars    0.641778  0.692497\n",
      "GarageArea    0.625583  0.648883\n",
      "TotalBsmtSF   0.604576  0.599329\n",
      "1stFlrSF      0.600848  0.574046\n",
      "FullBath      0.565798  0.640359\n",
      "TotRmsAbvGrd  0.528824  0.532821\n",
      "YearBuilt     0.516608  0.646911\n",
      "YearRemodAdd  0.510059  0.578902\n",
      "Fireplaces    0.470974  0.528224\n",
      "MasVnrArea    0.467104  0.416202\n",
      "GarageYrBlt   0.463615  0.563399\n",
      "BsmtFinSF1    0.367683  0.293354\n",
      "LotFrontage   0.344275  0.381988\n",
      "\n",
      " Top 10 Most Correlated Features:\n",
      " 1. OverallQual          = +0.792\n",
      " 2. GrLivArea            = +0.708\n",
      " 3. GarageCars           = +0.642\n",
      " 4. GarageArea           = +0.626\n",
      " 5. TotalBsmtSF          = +0.605\n",
      " 6. 1stFlrSF             = +0.601\n",
      " 7. FullBath             = +0.566\n",
      " 8. TotRmsAbvGrd         = +0.529\n",
      " 9. YearBuilt            = +0.517\n",
      "10. YearRemodAdd         = +0.510\n",
      " group_correlation_summary() defined\n",
      "\n",
      "  Correlation Summary by Feature Group:\n",
      "                    group  n_features  mean_abs_pearson  mean_abs_spearman  \\\n",
      "2     Quality & Condition           4          0.472645           0.540114   \n",
      "1         Size & Interior          14          0.387161           0.389744   \n",
      "0          Location & Lot           3          0.224217           0.287272   \n",
      "3  Amenities, Sale & Time           6          0.187872           0.217964   \n",
      "\n",
      "   max_abs_pearson  \n",
      "2         0.791936  \n",
      "1         0.707788  \n",
      "0         0.344275  \n",
      "3         0.470974  \n",
      "\n",
      "  Detailed Group Analysis:\n",
      "\n",
      "LOCATION & LOT (top 3):\n",
      "              pearson  spearman\n",
      "LotFrontage  0.344275  0.381988\n",
      "LotArea      0.256522  0.458749\n",
      "MSSubClass  -0.071855  0.021080\n",
      "\n",
      "SIZE & INTERIOR (top 3):\n",
      "             pearson  spearman\n",
      "GrLivArea   0.707788  0.732569\n",
      "GarageCars  0.641778  0.692497\n",
      "GarageArea  0.625583  0.648883\n",
      "\n",
      "QUALITY & CONDITION (top 3):\n",
      "               pearson  spearman\n",
      "OverallQual   0.791936  0.806647\n",
      "YearBuilt     0.516608  0.646911\n",
      "YearRemodAdd  0.510059  0.578902\n",
      "\n",
      "AMENITIES, SALE & TIME (top 3):\n",
      "              pearson  spearman\n",
      "Fireplaces   0.470974  0.528224\n",
      "GarageYrBlt  0.463615  0.563399\n",
      "PoolArea     0.098235  0.061467\n",
      "\n",
      "  Strong Correlations (|r| > 0.5): 10 features\n",
      "\n",
      "Top features:\n",
      "               pearson  spearman\n",
      "OverallQual   0.791936  0.806647\n",
      "GrLivArea     0.707788  0.732569\n",
      "GarageCars    0.641778  0.692497\n",
      "GarageArea    0.625583  0.648883\n",
      "TotalBsmtSF   0.604576  0.599329\n",
      "1stFlrSF      0.600848  0.574046\n",
      "FullBath      0.565798  0.640359\n",
      "TotRmsAbvGrd  0.528824  0.532821\n",
      "YearBuilt     0.516608  0.646911\n",
      "YearRemodAdd  0.510059  0.578902\n"
     ]
    }
   ],
   "source": [
    "def compute_correlations(df_in, target=\"SalePrice\"):\n",
    "    \"\"\"T√≠nh Pearson v√† Spearman correlation\"\"\"\n",
    "    numeric = df_in.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if target not in numeric:\n",
    "        raise ValueError(f\"{target} not in numeric columns\")\n",
    "    \n",
    "    numeric = [c for c in numeric if c != target]\n",
    "    \n",
    "    pearson_dict = {}\n",
    "    spearman_dict = {}\n",
    "    \n",
    "    for c in numeric:\n",
    "        try:\n",
    "            pearson_dict[c] = df_in[c].corr(df_in[target], method=\"pearson\")\n",
    "            spearman_dict[c] = df_in[c].corr(df_in[target], method=\"spearman\")\n",
    "        except:\n",
    "            pearson_dict[c] = np.nan\n",
    "            spearman_dict[c] = np.nan\n",
    "    \n",
    "    corr_df = pd.DataFrame({\n",
    "        \"pearson\": pd.Series(pearson_dict),\n",
    "        \"spearman\": pd.Series(spearman_dict)\n",
    "    })\n",
    "    \n",
    "    corr_df = corr_df.sort_values(by=\"pearson\", key=abs, ascending=False)\n",
    "    return corr_df\n",
    "\n",
    "print(\" compute_correlations() defined\")\n",
    "\n",
    "\n",
    "df_train_imputed = basic_impute(df_train)\n",
    "correlations = compute_correlations(df_train_imputed)\n",
    "\n",
    "print(\"  Top 15 Features by Pearson Correlation:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "\n",
    "top_features = correlations.head(10).index.tolist()\n",
    "print(\"\\n Top 10 Most Correlated Features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"{i:2d}. {feat:20s} = {correlations.loc[feat, 'pearson']:+.3f}\")\n",
    "\n",
    "\n",
    "def group_correlation_summary(corr_df, group_map):\n",
    "    \"\"\"T√≠nh mean absolute correlation cho t·ª´ng nh√≥m\"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for group_name, cols in group_map.items():\n",
    "        present = [c for c in cols if c in corr_df.index]\n",
    "        \n",
    "        if present:\n",
    "            mean_p = corr_df.loc[present, \"pearson\"].abs().mean()\n",
    "            mean_s = corr_df.loc[present, \"spearman\"].abs().mean()\n",
    "            max_p = corr_df.loc[present, \"pearson\"].abs().max()\n",
    "        else:\n",
    "            mean_p = mean_s = max_p = np.nan\n",
    "        \n",
    "        summary.append({\n",
    "            \"group\": group_name,\n",
    "            \"n_features\": len(present),\n",
    "            \"mean_abs_pearson\": mean_p,\n",
    "            \"mean_abs_spearman\": mean_s,\n",
    "            \"max_abs_pearson\": max_p\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary).sort_values(by=\"mean_abs_pearson\", ascending=False)\n",
    "\n",
    "print(\" group_correlation_summary() defined\")\n",
    "\n",
    "\n",
    "group_summary = group_correlation_summary(correlations, groups_info)\n",
    "\n",
    "print(\"\\n  Correlation Summary by Feature Group:\")\n",
    "print(group_summary)\n",
    "\n",
    "print(\"\\n  Detailed Group Analysis:\")\n",
    "for group_name, cols in groups_info.items():\n",
    "    present = [c for c in cols if c in correlations.index]\n",
    "    if present:\n",
    "        group_corr = correlations.loc[present].sort_values(\n",
    "            by=\"pearson\", key=abs, ascending=False\n",
    "        )\n",
    "        print(f\"\\n{group_name.upper()} (top 3):\")\n",
    "        print(group_corr.head(3))\n",
    "\n",
    "\n",
    "\n",
    "strong_corr = correlations[correlations['pearson'].abs() > 0.5]\n",
    "\n",
    "print(f\"\\n  Strong Correlations (|r| > 0.5): {len(strong_corr)} features\")\n",
    "print(\"\\nTop features:\")\n",
    "print(strong_corr.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e56882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PH√ÇN T√çCH 4 NH√ìM FEATURES ‚Äî FULL AUTOMATION, NO HARD CODE\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "1. T·ªîNG QUAN 4 NH√ìM FEATURES\n",
      "================================================================================\n",
      "\n",
      "Location & Lot\n",
      "   S·ªë features h·ª£p l·ªá: 14\n",
      "   V√≠ d·ª• 5 features: ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street']\n",
      "\n",
      "Size & Interior\n",
      "   S·ªë features h·ª£p l·ªá: 14\n",
      "   V√≠ d·ª• 5 features: ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF']\n",
      "\n",
      "Quality & Condition\n",
      "   S·ªë features h·ª£p l·ªá: 14\n",
      "   V√≠ d·ª• 5 features: ['OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual']\n",
      "\n",
      "Amenities, Sale & Time\n",
      "   S·ªë features h·ª£p l·ªá: 9\n",
      "   V√≠ d·ª• 5 features: ['Fireplaces', 'GarageYrBlt', 'PoolArea', 'Fence', 'MiscVal']\n",
      "\n",
      "================================================================================\n",
      "2. PH√ÇN T√çCH CORRELATION T·ª™NG NH√ìM\n",
      "================================================================================\n",
      "\n",
      "Location & Lot\n",
      "   Mean |corr|: 0.224\n",
      "   Top 5 strongest correlated:\n",
      "      LotFrontage               = +0.344\n",
      "      LotArea                   = +0.257\n",
      "      MSSubClass                = +0.072\n",
      "\n",
      "Size & Interior\n",
      "   Mean |corr|: 0.387\n",
      "   Top 5 strongest correlated:\n",
      "      GrLivArea                 = +0.708\n",
      "      GarageCars                = +0.642\n",
      "      GarageArea                = +0.626\n",
      "      TotalBsmtSF               = +0.605\n",
      "      1stFlrSF                  = +0.601\n",
      "\n",
      "Quality & Condition\n",
      "   Mean |corr|: 0.473\n",
      "   Top 5 strongest correlated:\n",
      "      OverallQual               = +0.792\n",
      "      YearBuilt                 = +0.517\n",
      "      YearRemodAdd              = +0.510\n",
      "      OverallCond               = +0.072\n",
      "\n",
      "Amenities, Sale & Time\n",
      "   Mean |corr|: 0.188\n",
      "   Top 5 strongest correlated:\n",
      "      Fireplaces                = +0.471\n",
      "      GarageYrBlt               = +0.464\n",
      "      PoolArea                  = +0.098\n",
      "      MoSold                    = +0.050\n",
      "      YrSold                    = +0.025\n",
      "\n",
      "================================================================================\n",
      "3. SO S√ÅNH S·ª®C M·∫†NH GI·ªÆA C√ÅC NH√ìM\n",
      "================================================================================\n",
      "                 Group  Mean_Abs_Corr Top_Feature  Top_Corr\n",
      "   Quality & Condition       0.472645 OverallQual  0.791936\n",
      "       Size & Interior       0.387161   GrLivArea  0.707788\n",
      "        Location & Lot       0.224217 LotFrontage  0.344275\n",
      "Amenities, Sale & Time       0.187872  Fireplaces  0.470974\n",
      "\n",
      "  Visual Ranking:\n",
      "Quality & Condition                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.473\n",
      "Size & Interior                     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.387\n",
      "Location & Lot                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.224\n",
      "Amenities, Sale & Time              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.188\n",
      "\n",
      "================================================================================\n",
      "4.1 CHI TI·∫æT NH√ìM 1 ‚Äî LOCATION & LOT\n",
      "================================================================================\n",
      "\n",
      "Categorical features: 11\n",
      "\n",
      "üìç MSZoning:\n",
      "   Unique: 5\n",
      "   Top 5 categories by mean price:\n",
      "      FV                  : $215,032\n",
      "      RL                  : $191,359\n",
      "      RH                  : $131,558\n",
      "      RM                  : $127,944\n",
      "      C (all)             : $74,016\n",
      "\n",
      "üìç Street:\n",
      "   Unique: 2\n",
      "   Top 5 categories by mean price:\n",
      "      Pave                : $181,488\n",
      "      Grvl                : $153,788\n",
      "\n",
      "üìç Alley:\n",
      "   Unique: 2\n",
      "   Top 5 categories by mean price:\n",
      "      Grvl                : $181,711\n",
      "      Pave                : $171,243\n",
      "\n",
      "================================================================================\n",
      "4.2 CHI TI·∫æT NH√ìM 2 ‚Äî SIZE & INTERIOR\n",
      "================================================================================\n",
      "\n",
      "Numeric features: 14\n",
      "\n",
      "Top 8 size features by mean value:\n",
      "                    mean         std    min     max\n",
      "GrLivArea    1520.329961  528.872138  334.0  5642.0\n",
      "1stFlrSF     1162.670039  387.449896  334.0  4692.0\n",
      "TotalBsmtSF  1058.329961  442.233182    0.0  6110.0\n",
      "BsmtUnfSF     571.667704  446.451727    0.0  2336.0\n",
      "GarageArea    475.801556  217.112236    0.0  1418.0\n",
      "BsmtFinSF1    442.515953  459.379404    0.0  5644.0\n",
      "2ndFlrSF      351.322957  437.559437    0.0  2065.0\n",
      "MasVnrArea    104.714397  184.610203    0.0  1600.0\n",
      "\n",
      "================================================================================\n",
      "4.3 CHI TI·∫æT NH√ìM 3 ‚Äî QUALITY & CONDITION\n",
      "================================================================================\n",
      "\n",
      " OverallQual vs Price:\n",
      "   Qual 1: $50,150 |  (2 homes)\n",
      "   Qual 2: $51,770 |  (3 homes)\n",
      "   Qual 3: $88,651 | ‚ñà (17 homes)\n",
      "   Qual 4: $109,054 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (98 homes)\n",
      "   Qual 5: $133,670 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (351 homes)\n",
      "   Qual 6: $162,608 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (329 homes)\n",
      "   Qual 7: $205,705 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (278 homes)\n",
      "   Qual 8: $274,897 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (152 homes)\n",
      "   Qual 9: $361,357 | ‚ñà‚ñà‚ñà (37 homes)\n",
      "   Qual 10: $438,588 | ‚ñà (18 homes)\n",
      "\n",
      "================================================================================\n",
      "4.4 CHI TI·∫æT NH√ìM 4 ‚Äî AMENITIES, SALE & TIME\n",
      "================================================================================\n",
      "\n",
      "Price by Year Sold:\n",
      "YrSold\n",
      "2006    182549.458599\n",
      "2007    186063.151976\n",
      "2008    177360.838816\n",
      "2009    179432.103550\n",
      "Name: SalePrice, dtype: float64\n",
      "\n",
      "Price by Sale Condition:\n",
      "   Partial        : $268,641\n",
      "   Alloca         : $177,503\n",
      "   Normal         : $175,009\n",
      "   Family         : $151,889\n",
      "   Abnorml        : $150,991\n",
      "   AdjLand        : $104,125\n",
      "\n",
      "================================================================================\n",
      "5. INSIGHTS T·ª∞ ƒê·ªòNG SINH T·ª™ 4 NH√ìM (NO HARD CODE)\n",
      "================================================================================\n",
      "\n",
      "Quality & Condition                 \n",
      "   ‚Üí Mean |corr|: 0.473\n",
      "   ‚Üí Strongest: OverallQual (+0.792)\n",
      "   ‚Üí Insight: Nh√≥m n√†y c√≥ t√°c ƒë·ªông r·∫•t m·∫°nh ƒë·∫øn SalePrice v√† c·∫ßn ∆∞u ti√™n trong modeling.\n",
      "\n",
      "Size & Interior                     \n",
      "   ‚Üí Mean |corr|: 0.387\n",
      "   ‚Üí Strongest: GrLivArea (+0.708)\n",
      "   ‚Üí Insight: Nh√≥m n√†y c√≥ ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ v√† ƒë√≥ng vai tr√≤ quan tr·ªçng.\n",
      "\n",
      "Location & Lot                      \n",
      "   ‚Üí Mean |corr|: 0.224\n",
      "   ‚Üí Strongest: LotFrontage (+0.344)\n",
      "   ‚Üí Insight: ·∫¢nh h∆∞·ªüng ·ªü m·ª©c trung b√¨nh, h·ªó tr·ª£ fine-tuning m√¥ h√¨nh.\n",
      "\n",
      "Amenities, Sale & Time              \n",
      "   ‚Üí Mean |corr|: 0.188\n",
      "   ‚Üí Strongest: Fireplaces (+0.471)\n",
      "   ‚Üí Insight: ·∫¢nh h∆∞·ªüng ·ªü m·ª©c trung b√¨nh, h·ªó tr·ª£ fine-tuning m√¥ h√¨nh.\n",
      "\n",
      "================================================================================\n",
      "FULL FEATURE GROUP ANALYSIS COMPLETED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_train_clean = basic_impute(df_train)\n",
    "corr_matrix = df_train_clean.corr(numeric_only=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"PH√ÇN T√çCH 4 NH√ìM FEATURES ‚Äî FULL AUTOMATION, NO HARD CODE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. T·ªîNG QUAN 4 NH√ìM FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, cols in groups_info.items():\n",
    "    valid = [c for c in cols if c in df_train_clean.columns]\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"   S·ªë features h·ª£p l·ªá: {len(valid)}\")\n",
    "    if len(valid) > 0:\n",
    "        print(f\"   V√≠ d·ª• 5 features: {valid[:5]}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Ph√¢n t√≠ch correlation t·ª´ng nh√≥m \n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. PH√ÇN T√çCH CORRELATION T·ª™NG NH√ìM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "group_corr_results = {}\n",
    "\n",
    "for name, cols in groups_info.items():\n",
    "    numeric_cols = [\n",
    "        c for c in cols \n",
    "        if c in corr_matrix.columns and c != 'SalePrice'\n",
    "    ]\n",
    "\n",
    "    if not numeric_cols:\n",
    "        print(f\"\\n  {name}: Kh√¥ng c√≥ numeric feature.\")\n",
    "        continue\n",
    "\n",
    "    corrs = corr_matrix.loc[numeric_cols, 'SalePrice']\n",
    "    corrs = corrs.dropna()\n",
    "\n",
    "    if corrs.empty:\n",
    "        print(f\"\\n  {name}: Kh√¥ng c√≥ gi√° tr·ªã correlation.\")\n",
    "        continue\n",
    "\n",
    "    corrs_sorted = corrs.abs().sort_values(ascending=False)\n",
    "\n",
    "    group_corr_results[name] = {\n",
    "        'mean_abs_corr': corrs_sorted.mean(),\n",
    "        'top_features': corrs_sorted.head(5)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"   Mean |corr|: {corrs_sorted.mean():.3f}\")\n",
    "    print(f\"   Top 5 strongest correlated:\")\n",
    "\n",
    "    for feat, val in corrs_sorted.head(5).items():\n",
    "        print(f\"      {feat:25s} = {val:+.3f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. So s√°nh s·ª©c m·∫°nh gi·ªØa c√°c nh√≥m \n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. SO S√ÅNH S·ª®C M·∫†NH GI·ªÆA C√ÅC NH√ìM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "group_strength_df = pd.DataFrame([\n",
    "    {\n",
    "        'Group': name,\n",
    "        'Mean_Abs_Corr': info['mean_abs_corr'],\n",
    "        'Top_Feature': info['top_features'].index[0],\n",
    "        'Top_Corr': info['top_features'].iloc[0]\n",
    "    }\n",
    "    for name, info in group_corr_results.items()\n",
    "]).sort_values('Mean_Abs_Corr', ascending=False)\n",
    "\n",
    "print(group_strength_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# Visualization\n",
    "print(\"\\n  Visual Ranking:\")\n",
    "for _, row in group_strength_df.iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Mean_Abs_Corr'] * 50)\n",
    "    print(f\"{row['Group']:35s} {bar} {row['Mean_Abs_Corr']:.3f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Ph√¢n t√≠ch chi ti·∫øt t·ª´ng nh√≥m \n",
    "# =============================================================================\n",
    "\n",
    "# ---------- Group 1 ----------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4.1 CHI TI·∫æT NH√ìM 1 ‚Äî LOCATION & LOT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cat_loc = [c for c in location_lot_cols if c in df_train_clean.columns and df_train_clean[c].dtype == 'object']\n",
    "\n",
    "print(f\"\\nCategorical features: {len(cat_loc)}\")\n",
    "\n",
    "for col in cat_loc[:3]:\n",
    "    print(f\"\\nüìç {col}:\")\n",
    "    vals = df_train_clean[col].value_counts()\n",
    "    print(f\"   Unique: {df_train_clean[col].nunique()}\")\n",
    "    if df_train_clean[col].nunique() < 50:\n",
    "        mean_price = df_train_clean.groupby(col)['SalePrice'].mean().sort_values(ascending=False).head(5)\n",
    "        print(\"   Top 5 categories by mean price:\")\n",
    "        for v, p in mean_price.items():\n",
    "            print(f\"      {v:20s}: ${p:,.0f}\")\n",
    "\n",
    "\n",
    "# ---------- Group 2 ----------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4.2 CHI TI·∫æT NH√ìM 2 ‚Äî SIZE & INTERIOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_size = [c for c in size_interior_cols if c in df_train_clean.columns and np.issubdtype(df_train_clean[c].dtype, np.number)]\n",
    "\n",
    "print(f\"\\nNumeric features: {len(num_size)}\")\n",
    "\n",
    "print(\"\\nTop 8 size features by mean value:\")\n",
    "print(df_train_clean[num_size].describe().T[['mean','std','min','max']].sort_values('mean', ascending=False).head(8))\n",
    "\n",
    "\n",
    "# ---------- Group 3 ----------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4.3 CHI TI·∫æT NH√ìM 3 ‚Äî QUALITY & CONDITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'OverallQual' in df_train_clean.columns:\n",
    "    qual_stat = df_train_clean.groupby('OverallQual')['SalePrice'].agg(['mean','count'])\n",
    "    print(\"\\n OverallQual vs Price:\")\n",
    "    for q, r in qual_stat.iterrows():\n",
    "        bar = \"‚ñà\" * int(r['count'] / 10)\n",
    "        print(f\"   Qual {q}: ${r['mean']:,.0f} | {bar} ({int(r['count'])} homes)\")\n",
    "\n",
    "\n",
    "# ---------- Group 4 ----------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4.4 CHI TI·∫æT NH√ìM 4 ‚Äî AMENITIES, SALE & TIME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'YrSold' in df_train_clean.columns:\n",
    "    print(\"\\nPrice by Year Sold:\")\n",
    "    print(df_train_clean.groupby('YrSold')['SalePrice'].mean())\n",
    "\n",
    "if 'SaleCondition' in df_train_clean.columns:\n",
    "    cond = df_train_clean.groupby('SaleCondition')['SalePrice'].mean().sort_values(ascending=False)\n",
    "    print(\"\\nPrice by Sale Condition:\")\n",
    "    for c, v in cond.items():\n",
    "        print(f\"   {c:15s}: ${v:,.0f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. AUTO-GENERATED INSIGHTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. INSIGHTS T·ª∞ ƒê·ªòNG SINH T·ª™ 4 NH√ìM (NO HARD CODE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def auto_insight(group_name, mean_corr):\n",
    "    if mean_corr > 0.45:\n",
    "        return \"Nh√≥m n√†y c√≥ t√°c ƒë·ªông r·∫•t m·∫°nh ƒë·∫øn SalePrice v√† c·∫ßn ∆∞u ti√™n trong modeling.\"\n",
    "    if mean_corr > 0.30:\n",
    "        return \"Nh√≥m n√†y c√≥ ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ v√† ƒë√≥ng vai tr√≤ quan tr·ªçng.\"\n",
    "    if mean_corr > 0.15:\n",
    "        return \"·∫¢nh h∆∞·ªüng ·ªü m·ª©c trung b√¨nh, h·ªó tr·ª£ fine-tuning m√¥ h√¨nh.\"\n",
    "    return \"·∫¢nh h∆∞·ªüng th·∫•p, mang t√≠nh b·ªï sung.\"\n",
    "\n",
    "for _, row in group_strength_df.iterrows():\n",
    "    strength = row['Mean_Abs_Corr']\n",
    "    stars = \"\" * max(1, min(5, int(strength * 10)))\n",
    "\n",
    "    print(f\"\\n{row['Group']:35s} {stars}\")\n",
    "    print(f\"   ‚Üí Mean |corr|: {strength:.3f}\")\n",
    "    print(f\"   ‚Üí Strongest: {row['Top_Feature']} ({row['Top_Corr']:+.3f})\")\n",
    "    print(f\"   ‚Üí Insight: {auto_insight(row['Group'], strength)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL FEATURE GROUP ANALYSIS COMPLETED\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to_monthly_timeseries() defined\n",
      "\n",
      "  Monthly Time Series:\n",
      "   Train: 48 months\n",
      "   Test: 7 months\n",
      "   Exogenous features: 9\n",
      " Train and test have matching columns\n",
      "\n",
      "  Train Time Series (first 5 months):\n",
      "          ds              y\n",
      "0 2006-01-01  201090.000000\n",
      "1 2006-02-01  194322.222222\n",
      "2 2006-03-01  184982.200000\n",
      "3 2006-04-01  174312.814815\n",
      "4 2006-05-01  158928.289474\n",
      "\n",
      "Train Time Series (last 5 months):\n",
      "           ds              y\n",
      "43 2009-08-01  165670.966667\n",
      "44 2009-09-01  196849.350000\n",
      "45 2009-10-01  175206.592593\n",
      "46 2009-11-01  156381.818182\n",
      "47 2009-12-01  164014.533333\n",
      "\n",
      "  Exogenous Features (first 5 months):\n",
      "            OverallQual_mean  GrLivArea_mean  TotRmsAbvGrd_mean  \\\n",
      "2006-01-01          6.000000     1517.500000           6.400000   \n",
      "2006-02-01          6.777778     1758.555556           7.111111   \n",
      "2006-03-01          6.360000     1558.440000           6.720000   \n",
      "2006-04-01          6.259259     1541.777778           6.518519   \n",
      "2006-05-01          5.605263     1365.605263           6.210526   \n",
      "\n",
      "            SaleCond_Abnorml  SaleCond_AdjLand  SaleCond_Alloca  \\\n",
      "2006-01-01          0.000000          0.000000              0.0   \n",
      "2006-02-01          0.222222          0.000000              0.0   \n",
      "2006-03-01          0.120000          0.000000              0.0   \n",
      "2006-04-01          0.148148          0.000000              0.0   \n",
      "2006-05-01          0.026316          0.026316              0.0   \n",
      "\n",
      "            SaleCond_Family  SaleCond_Normal  SaleCond_Partial  \n",
      "2006-01-01             0.10         0.800000          0.100000  \n",
      "2006-02-01             0.00         0.777778          0.000000  \n",
      "2006-03-01             0.04         0.640000          0.200000  \n",
      "2006-04-01             0.00         0.777778          0.074074  \n",
      "2006-05-01             0.00         0.842105          0.105263  \n",
      "\n",
      "  Test Time Series:\n",
      "          ds              y\n",
      "0 2010-01-01  163852.600000\n",
      "1 2010-02-01  174823.333333\n",
      "2 2010-03-01  203181.285714\n",
      "3 2010-04-01  171344.025641\n",
      "4 2010-05-01  178422.250000\n",
      "\n",
      "Test period: 2010-01-01 00:00:00 to 2010-07-01 00:00:00\n",
      "\n",
      "  Time Series Statistics:\n",
      "   Train mean price: $182,932.66\n",
      "   Train std: $16,220.60\n",
      "   Test mean price: $170,716.15\n",
      "   Test std: $24,815.14\n",
      "\n",
      "  Missing Values Check:\n",
      "   ts_train: 0 NaN\n",
      "   ts_test: 0 NaN\n",
      "   exog_train: 0 NaN\n",
      "   exog_test: 0 NaN\n",
      " create_lstm_windows() defined\n",
      "\n",
      "  LSTM Windows:\n",
      "   X shape: (36, 12, 10)\n",
      "   y shape: (36,)\n",
      "   Window size: 12 months\n",
      "   Features per timestep: 10\n",
      "\n",
      "  Sample LSTM Window (first window):\n",
      "   Input shape: (12, 10)\n",
      "   Output value: 0.3985\n"
     ]
    }
   ],
   "source": [
    "def to_monthly_timeseries(df_in, reference_columns=None):\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi sang monthly aggregation\"\"\"\n",
    "    df = df_in.copy()\n",
    "    \n",
    "    if \"YrSold\" not in df.columns or \"MoSold\" not in df.columns:\n",
    "        raise ValueError(\"Missing YrSold or MoSold columns\")\n",
    "    \n",
    "    # Create datetime index\n",
    "    df[\"date\"] = pd.to_datetime(\n",
    "        df[\"YrSold\"].astype(int).astype(str) + \"-\" +\n",
    "        df[\"MoSold\"].astype(int).astype(str) + \"-01\"\n",
    "    )\n",
    "    df = df.set_index(\"date\")\n",
    "    \n",
    "    # Aggregate target (mean price per month)\n",
    "    monthly_target = df[\"SalePrice\"].resample(\"MS\").mean()\n",
    "    \n",
    "    # Aggregate exogenous features\n",
    "    exog = pd.DataFrame(index=monthly_target.index)\n",
    "    \n",
    "    # Key numeric features\n",
    "    if \"OverallQual\" in df.columns:\n",
    "        exog[\"OverallQual_mean\"] = df[\"OverallQual\"].resample(\"MS\").mean()\n",
    "    \n",
    "    if \"GrLivArea\" in df.columns:\n",
    "        exog[\"GrLivArea_mean\"] = df[\"GrLivArea\"].resample(\"MS\").mean()\n",
    "    \n",
    "    if \"TotRmsAbvGrd\" in df.columns:\n",
    "        exog[\"TotRmsAbvGrd_mean\"] = df[\"TotRmsAbvGrd\"].resample(\"MS\").mean()\n",
    "    \n",
    "    # Categorical features (proportions)\n",
    "    if \"SaleCondition\" in df.columns:\n",
    "        salecond = pd.get_dummies(df[\"SaleCondition\"].astype(str))\n",
    "        salecond_monthly = salecond.resample(\"MS\").mean()\n",
    "        salecond_monthly = salecond_monthly.add_prefix(\"SaleCond_\")\n",
    "        exog = pd.concat([exog, salecond_monthly], axis=1)\n",
    "    \n",
    "    # If reference columns provided, align to them\n",
    "    if reference_columns is not None:\n",
    "        for col in reference_columns:\n",
    "            if col not in exog.columns:\n",
    "                exog[col] = 0\n",
    "        exog = exog[reference_columns]\n",
    "    \n",
    "    # Create full date range\n",
    "    full_range = pd.date_range(\n",
    "        start=monthly_target.index.min(),\n",
    "        end=monthly_target.index.max(),\n",
    "        freq=\"MS\"\n",
    "    )\n",
    "    \n",
    "    # Reindex\n",
    "    monthly_target = monthly_target.reindex(full_range)\n",
    "    exog = exog.reindex(full_range)\n",
    "    \n",
    "    # Interpolate missing\n",
    "    monthly_target = monthly_target.interpolate(method=\"linear\").ffill().bfill()\n",
    "    exog = exog.interpolate(method=\"linear\").ffill().bfill()\n",
    "    \n",
    "    # Fill any remaining NaN\n",
    "    for col in exog.columns:\n",
    "        if exog[col].isna().any():\n",
    "            exog[col] = exog[col].fillna(exog[col].mean())\n",
    "    \n",
    "    # Format for Prophet\n",
    "    ts_df = pd.DataFrame({\n",
    "        \"ds\": monthly_target.index,\n",
    "        \"y\": monthly_target.values\n",
    "    })\n",
    "    \n",
    "    return ts_df, exog\n",
    "\n",
    "print(\" to_monthly_timeseries() defined\")\n",
    "\n",
    "\n",
    "# Create monthly time series\n",
    "ts_train, exog_train = to_monthly_timeseries(df_train)\n",
    "\n",
    "# Create test with same columns as train\n",
    "ts_test, exog_test = to_monthly_timeseries(df_test, reference_columns=exog_train.columns)\n",
    "\n",
    "print(\"\\n  Monthly Time Series:\")\n",
    "print(f\"   Train: {len(ts_train)} months\")\n",
    "print(f\"   Test: {len(ts_test)} months\")\n",
    "print(f\"   Exogenous features: {len(exog_train.columns)}\")\n",
    "\n",
    "# Verify columns match\n",
    "assert list(exog_train.columns) == list(exog_test.columns), \"Column mismatch!\"\n",
    "print(\" Train and test have matching columns\")\n",
    "\n",
    "# Show train series\n",
    "print(\"\\n  Train Time Series (first 5 months):\")\n",
    "print(ts_train.head())\n",
    "\n",
    "print(\"\\nTrain Time Series (last 5 months):\")\n",
    "print(ts_train.tail())\n",
    "\n",
    "\n",
    "# Show exogenous features\n",
    "print(\"\\n  Exogenous Features (first 5 months):\")\n",
    "print(exog_train.head())\n",
    "\n",
    "\n",
    "# Show test series\n",
    "print(\"\\n  Test Time Series:\")\n",
    "print(ts_test.head())\n",
    "print(f\"\\nTest period: {ts_test['ds'].min()} to {ts_test['ds'].max()}\")\n",
    "\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n  Time Series Statistics:\")\n",
    "print(f\"   Train mean price: ${ts_train['y'].mean():,.2f}\")\n",
    "print(f\"   Train std: ${ts_train['y'].std():,.2f}\")\n",
    "print(f\"   Test mean price: ${ts_test['y'].mean():,.2f}\")\n",
    "print(f\"   Test std: ${ts_test['y'].std():,.2f}\")\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n  Missing Values Check:\")\n",
    "print(f\"   ts_train: {ts_train['y'].isna().sum()} NaN\")\n",
    "print(f\"   ts_test: {ts_test['y'].isna().sum()} NaN\")\n",
    "print(f\"   exog_train: {exog_train.isna().sum().sum()} NaN\")\n",
    "print(f\"   exog_test: {exog_test.isna().sum().sum()} NaN\")\n",
    "\n",
    "\n",
    "def create_lstm_windows(series_df, exog_df=None, lags=12):\n",
    "    \"\"\"T·∫°o sliding windows cho LSTM\"\"\"\n",
    "    s = series_df.copy().reset_index(drop=True)\n",
    "    ex = exog_df.copy().reset_index(drop=True) if exog_df is not None else None\n",
    "    \n",
    "    values = s[\"y\"].values\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for i in range(len(values) - lags):\n",
    "        # Target window\n",
    "        x_window = values[i:i+lags]\n",
    "        y_val = values[i+lags]\n",
    "        \n",
    "        # Add exogenous features\n",
    "        if ex is not None:\n",
    "            ex_window = ex.iloc[i:i+lags].values\n",
    "            x_combined = np.hstack([\n",
    "                x_window.reshape(-1, 1),\n",
    "                ex_window\n",
    "            ])\n",
    "        else:\n",
    "            x_combined = x_window.reshape(-1, 1)\n",
    "        \n",
    "        X_list.append(x_combined)\n",
    "        y_list.append(y_val)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\" create_lstm_windows() defined\")\n",
    "\n",
    "# Scale target for LSTM\n",
    "scaler_y = MinMaxScaler()\n",
    "ts_train_scaled = ts_train.copy()\n",
    "ts_train_scaled[\"y\"] = scaler_y.fit_transform(ts_train[[\"y\"]])\n",
    "\n",
    "# Create LSTM windows\n",
    "X_lstm_train, y_lstm_train = create_lstm_windows(\n",
    "    ts_train_scaled,\n",
    "    exog_df=exog_train,\n",
    "    lags=12\n",
    ")\n",
    "\n",
    "print(\"\\n  LSTM Windows:\")\n",
    "print(f\"   X shape: {X_lstm_train.shape}\")\n",
    "print(f\"   y shape: {y_lstm_train.shape}\")\n",
    "print(f\"   Window size: 12 months\")\n",
    "print(f\"   Features per timestep: {X_lstm_train.shape[2]}\")\n",
    "\n",
    "# Show sample window\n",
    "print(\"\\n  Sample LSTM Window (first window):\")\n",
    "print(f\"   Input shape: {X_lstm_train[0].shape}\")\n",
    "print(f\"   Output value: {y_lstm_train[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0079af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Checking data quality...\n",
      "   ts_train NaN: 0\n",
      "   exog_train NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Added 9 regressors\n",
      "\n",
      " Fitting Prophet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model fitted\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "  Prophet Results:\n",
      "   RMSE: $18,670.57\n",
      "   MAE: $14,850.68\n",
      "   MAPE: 9.59%\n",
      "\n",
      "  Predictions vs Actual (first 5):\n",
      "        Date         Actual      Predicted         Error    Error_%\n",
      "0 2010-01-01  163852.600000  186575.934880 -22723.334880 -13.868156\n",
      "1 2010-02-01  174823.333333  163357.686867  11465.646466   6.558419\n",
      "2 2010-03-01  203181.285714  181247.127209  21934.158505  10.795364\n",
      "3 2010-04-01  171344.025641  171544.564265   -200.538624  -0.117039\n",
      "4 2010-05-01  178422.250000  171300.351953   7121.898047   3.991597\n",
      "\n",
      "  Error Statistics:\n",
      "   Mean Error: $-1,723.63\n",
      "   Std Error: $20,080.40\n",
      "   Mean Absolute Error %: 9.59%\n",
      "\n",
      "  Forecast Components:\n",
      "           ds          trend        yearly\n",
      "43 2009-08-01  176631.588353   8508.731065\n",
      "44 2009-09-01  176537.424116   4951.720849\n",
      "45 2009-10-01  176446.297434  -2284.256042\n",
      "46 2009-11-01  176352.133197  12417.765627\n",
      "47 2009-12-01  176261.006515  -3647.312127\n",
      "\n",
      "==================================================\n",
      "PROPHET MODEL SUMMARY\n",
      "==================================================\n",
      "Training period: 2006-01-01 00:00:00 to 2009-12-01 00:00:00\n",
      "Test period: 2010-01-01 00:00:00 to 2010-07-01 00:00:00\n",
      "\n",
      "Performance Metrics:\n",
      "  RMSE: $18,670.57\n",
      "  MAE: $14,850.68\n",
      "  MAPE: 9.59%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def fit_prophet_model(ts_train, exog_train, ts_test, exog_test):\n",
    "    \"\"\"Fit Prophet model v·ªõi exogenous regressors\"\"\"\n",
    "    \n",
    "    if Prophet is None:\n",
    "        print(\"  Prophet not available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"  Checking data quality...\")\n",
    "    print(f\"   ts_train NaN: {ts_train['y'].isna().sum()}\")\n",
    "    print(f\"   exog_train NaN: {exog_train.isna().sum().sum()}\")\n",
    "    \n",
    "    # Initialize Prophet\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05\n",
    "    )\n",
    "    \n",
    "    # Add regressors\n",
    "    for col in exog_train.columns:\n",
    "        m.add_regressor(col)\n",
    "    \n",
    "    print(f\"\\n Added {len(exog_train.columns)} regressors\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    df_train = ts_train.copy().reset_index(drop=True)\n",
    "    exog_train_reset = exog_train.reset_index(drop=True)\n",
    "    df_fit = pd.concat([df_train, exog_train_reset], axis=1)\n",
    "    \n",
    "    # Fill any NaN\n",
    "    if df_fit.isna().any().any():\n",
    "        print(\" Filling NaN in training data...\")\n",
    "        df_fit = df_fit.fillna(df_fit.mean())\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"\\n Fitting Prophet model...\")\n",
    "    m.fit(df_fit)\n",
    "    print(\" Model fitted\")\n",
    "    \n",
    "    return m\n",
    "\n",
    "# %%\n",
    "# Train Prophet\n",
    "if Prophet is not None:\n",
    "    prophet_model = fit_prophet_model(ts_train, exog_train, ts_test, exog_test)\n",
    "else:\n",
    "    prophet_model = None\n",
    "    print(\"  Skipping Prophet (not installed)\")\n",
    "\n",
    "\n",
    "def evaluate_prophet(model, ts_test, exog_test):\n",
    "    \"\"\"Evaluate Prophet tr√™n test set\"\"\"\n",
    "    \n",
    "    # Prepare test data\n",
    "    df_test = ts_test.copy().reset_index(drop=True)\n",
    "    exog_test_reset = exog_test.reset_index(drop=True)\n",
    "    df_test_full = pd.concat([df_test[['ds']], exog_test_reset], axis=1)\n",
    "    \n",
    "    # Fill NaN\n",
    "    if df_test_full.isna().any().any():\n",
    "        print(\"  Filling NaN in test data...\")\n",
    "        df_test_full = df_test_full.fillna(exog_test_reset.mean())\n",
    "    \n",
    "    # Predict\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    forecast = model.predict(df_test_full)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true = ts_test[\"y\"].values\n",
    "    y_pred = forecast[\"yhat\"].values[:len(y_true)]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        \"forecast\": forecast,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate Prophet\n",
    "if prophet_model is not None:\n",
    "    prophet_results = evaluate_prophet(prophet_model, ts_test, exog_test)\n",
    "    \n",
    "    print(\"\\n  Prophet Results:\")\n",
    "    print(f\"   RMSE: ${prophet_results['rmse']:,.2f}\")\n",
    "    print(f\"   MAE: ${prophet_results['mae']:,.2f}\")\n",
    "    print(f\"   MAPE: {prophet_results['mape']:.2f}%\")\n",
    "else:\n",
    "    prophet_results = None\n",
    "\n",
    "\n",
    "# Show predictions vs actual\n",
    "if prophet_results is not None:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Date': ts_test['ds'].values,\n",
    "        'Actual': prophet_results['y_true'],\n",
    "        'Predicted': prophet_results['y_pred'],\n",
    "        'Error': prophet_results['y_true'] - prophet_results['y_pred'],\n",
    "        'Error_%': ((prophet_results['y_true'] - prophet_results['y_pred']) / \n",
    "                    prophet_results['y_true'] * 100)\n",
    "    })\n",
    "    \n",
    "    print(\"\\n  Predictions vs Actual (first 5):\")\n",
    "    print(comparison.head())\n",
    "    \n",
    "    print(\"\\n  Error Statistics:\")\n",
    "    print(f\"   Mean Error: ${comparison['Error'].mean():,.2f}\")\n",
    "    print(f\"   Std Error: ${comparison['Error'].std():,.2f}\")\n",
    "    print(f\"   Mean Absolute Error %: {comparison['Error_%'].abs().mean():.2f}%\")\n",
    "\n",
    "\n",
    "if prophet_model is not None:\n",
    "    # Get components\n",
    "    future = prophet_model.make_future_dataframe(periods=0, freq='MS')\n",
    "    \n",
    "    # Add regressors to future\n",
    "    exog_full = pd.concat([exog_train, exog_test]).reset_index(drop=True)\n",
    "    for col in exog_train.columns:\n",
    "        future[col] = exog_full[col].values[:len(future)]\n",
    "    \n",
    "    forecast_full = prophet_model.predict(future)\n",
    "    \n",
    "    print(\"\\n  Forecast Components:\")\n",
    "    print(forecast_full[['ds', 'trend', 'yearly']].tail())\n",
    "\n",
    "\n",
    "if prophet_results is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROPHET MODEL SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Training period: {ts_train['ds'].min()} to {ts_train['ds'].max()}\")\n",
    "    print(f\"Test period: {ts_test['ds'].min()} to {ts_test['ds'].max()}\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  RMSE: ${prophet_results['rmse']:,.2f}\")\n",
    "    print(f\"  MAE: ${prophet_results['mae']:,.2f}\")\n",
    "    print(f\"  MAPE: {prophet_results['mape']:.2f}%\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting SARIMAX(1,1,1)x(1,1,1,12)...\n",
      " SARIMAX fitted successfully\n",
      "\n",
      "  SARIMAX Model Information:\n",
      "   AIC: 490.37\n",
      "   BIC: 505.00\n",
      "\n",
      "  Model Parameters (first 10):\n",
      "OverallQual_mean     29551.214864\n",
      "GrLivArea_mean         -20.278695\n",
      "TotRmsAbvGrd_mean    16090.252344\n",
      "SaleCond_Abnorml    -17935.374897\n",
      "SaleCond_AdjLand     34249.609225\n",
      "SaleCond_Alloca     -39546.453081\n",
      "SaleCond_Family       -264.697633\n",
      "SaleCond_Normal     -22221.174124\n",
      "SaleCond_Partial     45718.090116\n",
      "ar.L1                   -0.318855\n",
      "dtype: float64\n",
      "\n",
      "  Model Summary:\n",
      "   Log Likelihood: -231.19\n",
      "   AIC: 490.37\n",
      "   BIC: 505.00\n",
      "   HQIC: 493.55\n",
      "\n",
      "  Residual Diagnostics:\n",
      "   Mean: 367.9125\n",
      "   Std: 18439.99\n",
      "   Min: -35190.38\n",
      "   Max: 44783.39\n",
      "\n",
      "  SARIMAX Test Performance:\n",
      "   RMSE: $14,379.86\n",
      "   MAE: $12,791.10\n",
      "   MAPE: 8.09%\n",
      "\n",
      "  SARIMAX Predictions (first 5):\n",
      "        Date         Actual      Predicted         Error\n",
      "0 2010-01-01  163852.600000  182001.545790 -18148.945790\n",
      "1 2010-02-01  174823.333333  163108.455690  11714.877643\n",
      "2 2010-03-01  203181.285714  189763.917187  13417.368527\n",
      "3 2010-04-01  171344.025641  161066.655868  10277.369773\n",
      "4 2010-05-01  178422.250000  177541.084357    881.165643\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Prophet:\n",
      "  RMSE: $18,670.57\n",
      "  MAE: $14,850.68\n",
      "  MAPE: 9.59%\n",
      "\n",
      "SARIMAX:\n",
      "  RMSE: $14,379.86\n",
      "  MAE: $12,791.10\n",
      "  MAPE: 8.09%\n",
      "  AIC: 490.37\n",
      "  BIC: 505.00\n",
      "============================================================\n",
      "\n",
      " Best Model: SARIMAX\n",
      "   Improvement: 22.98%\n"
     ]
    }
   ],
   "source": [
    "def fit_sarimax_model(ts_train, exog_train):\n",
    "    \"\"\"Fit SARIMAX model v√† t√≠nh AIC/BIC\"\"\"\n",
    "    \n",
    "    if sm is None:\n",
    "        print(\"  statsmodels not available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        s = ts_train.set_index(\"ds\")[\"y\"]\n",
    "        \n",
    "        print(\"  Fitting SARIMAX(1,1,1)x(1,1,1,12)...\")\n",
    "        \n",
    "        # SARIMAX model\n",
    "        mod = sm.tsa.statespace.SARIMAX(\n",
    "            s,\n",
    "            exog=exog_train,\n",
    "            order=(1, 1, 1),\n",
    "            seasonal_order=(1, 1, 1, 12),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        # Fit\n",
    "        res = mod.fit(disp=False, maxiter=200)\n",
    "        \n",
    "        print(\" SARIMAX fitted successfully\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": res,\n",
    "            \"aic\": res.aic,\n",
    "            \"bic\": res.bic,\n",
    "            \"params\": res.params\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  SARIMAX error: {e}\")\n",
    "        return None\n",
    "\n",
    "sarimax_results = fit_sarimax_model(ts_train, exog_train)\n",
    "\n",
    "if sarimax_results is not None:\n",
    "    print(\"\\n  SARIMAX Model Information:\")\n",
    "    print(f\"   AIC: {sarimax_results['aic']:.2f}\")\n",
    "    print(f\"   BIC: {sarimax_results['bic']:.2f}\")\n",
    "    \n",
    "    print(\"\\n  Model Parameters (first 10):\")\n",
    "    print(sarimax_results['params'].head(10))\n",
    "\n",
    "\n",
    "if sarimax_results is not None:\n",
    "    model = sarimax_results['model']\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n  Model Summary:\")\n",
    "    print(f\"   Log Likelihood: {model.llf:.2f}\")\n",
    "    print(f\"   AIC: {model.aic:.2f}\")\n",
    "    print(f\"   BIC: {model.bic:.2f}\")\n",
    "    print(f\"   HQIC: {model.hqic:.2f}\")\n",
    "\n",
    "\n",
    "if sarimax_results is not None:\n",
    "    residuals = sarimax_results['model'].resid\n",
    "    \n",
    "    print(\"\\n  Residual Diagnostics:\")\n",
    "    print(f\"   Mean: {residuals.mean():.4f}\")\n",
    "    print(f\"   Std: {residuals.std():.2f}\")\n",
    "    print(f\"   Min: {residuals.min():.2f}\")\n",
    "    print(f\"   Max: {residuals.max():.2f}\")\n",
    "\n",
    "\n",
    "def predict_sarimax(model_result, exog_test, steps):\n",
    "    \"\"\"Make predictions v·ªõi SARIMAX\"\"\"\n",
    "    try:\n",
    "        # Forecast\n",
    "        forecast = model_result['model'].forecast(\n",
    "            steps=steps,\n",
    "            exog=exog_test.iloc[:steps]\n",
    "        )\n",
    "        \n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        print(f\"  Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "if sarimax_results is not None:\n",
    "    # Predict\n",
    "    n_test = len(ts_test)\n",
    "    sarimax_pred = predict_sarimax(sarimax_results, exog_test, n_test)\n",
    "    \n",
    "    if sarimax_pred is not None:\n",
    "        # Calculate metrics\n",
    "        y_true = ts_test['y'].values\n",
    "        y_pred = sarimax_pred.values\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        print(\"\\n  SARIMAX Test Performance:\")\n",
    "        print(f\"   RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"   MAE: ${mae:,.2f}\")\n",
    "        print(f\"   MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # Store results\n",
    "        sarimax_results['test_metrics'] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "\n",
    "\n",
    "if sarimax_results is not None and 'test_metrics' in sarimax_results:\n",
    "    metrics = sarimax_results['test_metrics']\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Date': ts_test['ds'].values,\n",
    "        'Actual': metrics['y_true'],\n",
    "        'Predicted': metrics['y_pred'],\n",
    "        'Error': metrics['y_true'] - metrics['y_pred']\n",
    "    })\n",
    "    \n",
    "    print(\"\\n  SARIMAX Predictions (first 5):\")\n",
    "    print(comparison.head())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if prophet_results is not None:\n",
    "    print(\"\\nProphet:\")\n",
    "    print(f\"  RMSE: ${prophet_results['rmse']:,.2f}\")\n",
    "    print(f\"  MAE: ${prophet_results['mae']:,.2f}\")\n",
    "    print(f\"  MAPE: {prophet_results['mape']:.2f}%\")\n",
    "\n",
    "if sarimax_results is not None and 'test_metrics' in sarimax_results:\n",
    "    metrics = sarimax_results['test_metrics']\n",
    "    print(\"\\nSARIMAX:\")\n",
    "    print(f\"  RMSE: ${metrics['rmse']:,.2f}\")\n",
    "    print(f\"  MAE: ${metrics['mae']:,.2f}\")\n",
    "    print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "    print(f\"  AIC: {sarimax_results['aic']:.2f}\")\n",
    "    print(f\"  BIC: {sarimax_results['bic']:.2f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if prophet_results is not None and sarimax_results is not None:\n",
    "    if 'test_metrics' in sarimax_results:\n",
    "        prophet_rmse = prophet_results['rmse']\n",
    "        sarimax_rmse = sarimax_results['test_metrics']['rmse']\n",
    "        \n",
    "        best_model = \"Prophet\" if prophet_rmse < sarimax_rmse else \"SARIMAX\"\n",
    "        improvement = abs(prophet_rmse - sarimax_rmse) / max(prophet_rmse, sarimax_rmse) * 100\n",
    "        \n",
    "        print(f\"\\n Best Model: {best_model}\")\n",
    "        print(f\"   Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scaled training data\n",
      " Created sequences: X=(36, 12, 1), y=(36, 1)\n",
      " LSTM model built\n",
      "Epoch 1/200\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.1596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 392ms/step - loss: 0.1550 - val_loss: 0.0683\n",
      "Epoch 2/200\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1054 - val_loss: 0.0532\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0692 - val_loss: 0.0597\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0502 - val_loss: 0.0825\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0629 - val_loss: 0.1065\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0701 - val_loss: 0.0991\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0687 - val_loss: 0.0802\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0691 - val_loss: 0.0644\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0544 - val_loss: 0.0568\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0549 - val_loss: 0.0542\n",
      "Epoch 11/200\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0583 - val_loss: 0.0531\n",
      "Epoch 12/200\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0566 - val_loss: 0.0529\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0626 - val_loss: 0.0534\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0614 - val_loss: 0.0548\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0503 - val_loss: 0.0578\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0533 - val_loss: 0.0620\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0586 - val_loss: 0.0665\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0474 - val_loss: 0.0705\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0634 - val_loss: 0.0713\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0489 - val_loss: 0.0699\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0474 - val_loss: 0.0684\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0560 - val_loss: 0.0657\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0499 - val_loss: 0.0628\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0542 - val_loss: 0.0613\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0522 - val_loss: 0.0602\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0575 - val_loss: 0.0597\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0486 - val_loss: 0.0604\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0537 - val_loss: 0.0615\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0516 - val_loss: 0.0639\n",
      "Epoch 30/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0450 - val_loss: 0.0657\n",
      "Epoch 31/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0584 - val_loss: 0.0670\n",
      "Epoch 32/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0547 - val_loss: 0.0668\n",
      " Training completed\n",
      " Best model saved automatically to best_model/lstm_best_model.h5\n",
      " Scaler saved: best_model/lstm_scaler.pkl\n",
      " Metadata saved: best_model/metadata.json\n",
      "\n",
      "========================================\n",
      "LSTM TRAINING + BEST MODEL EXPORT DONE\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "train_values = ts_train['y'].values.reshape(-1, 1)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "train_scaled = scaler_y.fit_transform(train_values)\n",
    "\n",
    "print(\" Scaled training data\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CREATE SEQUENCES\n",
    "# =============================================================================\n",
    "def create_sequences(data, seq_len=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LEN = 12\n",
    "X_train, y_train = create_sequences(train_scaled, SEQ_LEN)\n",
    "\n",
    "print(f\" Created sequences: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. BUILD LSTM MODEL\n",
    "# =============================================================================\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "print(\" LSTM model built\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TRAIN WITH CALLBACKS (BEST MODEL AUTO SAVE)\n",
    "# =============================================================================\n",
    "\n",
    "os.makedirs(\"best_model\", exist_ok=True)\n",
    "best_model_path = \"best_model/lstm_best_model.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=20, restore_best_weights=True),\n",
    "    ModelCheckpoint(best_model_path, save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\" Training completed\")\n",
    "print(f\" Best model saved automatically to {best_model_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SAVE SCALER\n",
    "# =============================================================================\n",
    "\n",
    "with open(\"best_model/lstm_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "\n",
    "print(\" Scaler saved: best_model/lstm_scaler.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SAVE METADATA\n",
    "# =============================================================================\n",
    "\n",
    "metadata = {\n",
    "    \"sequence_length\": SEQ_LEN,\n",
    "    \"n_features\": 1,\n",
    "    \"target\": \"SalePrice\",\n",
    "    \"model_type\": \"LSTM\"\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"best_model/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\" Metadata saved: best_model/metadata.json\")\n",
    "\n",
    "# =============================================================================\n",
    "# DONE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"LSTM TRAINING + BEST MODEL EXPORT DONE\")\n",
    "print(\"========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70615264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STRATEGIES OVERVIEW ===\n",
      "             Strategy Train_Shape Test_Shape  N_Features                          Description\n",
      " 1. OneHot + Standard (1285, 287) (175, 287)         286    One-Hot Encoding + StandardScaler\n",
      "   2. Target + Robust  (1285, 81)  (175, 81)          80       Target Encoding + RobustScaler\n",
      "     3. PCA + Ordinal  (1285, 37)  (175, 37)          36               PCA + Ordinal Encoding\n",
      "4. MinMax + Embedding (1285, 235) (175, 235)         234 MinMaxScaler + Embedding Preparation\n",
      "\n",
      "=== FEATURE QUALITY ===\n",
      "  strategy  n_features  n_missing  n_constant  n_high_corr_pairs  memory_mb\n",
      "Strategy 1         286          0           0                 10   0.677686\n",
      "Strategy 2          80          0           0                  2   0.794106\n",
      "Strategy 3          36          0           0                  0   0.313721\n",
      "Strategy 4         234          0           0                  6   0.639696\n",
      "\n",
      "=== MODEL PERFORMANCE (Ridge) ===\n",
      "  strategy      cv_rmse    test_rmse     test_mae  features_used\n",
      "Strategy 1 34090.518471 5.333553e+04 4.877329e+04            286\n",
      "Strategy 2 38181.057095 1.045942e+08 1.045836e+08             80\n",
      "Strategy 3 36168.303126 1.624179e+05 1.594893e+05             36\n",
      "Strategy 4 35710.304091 1.171450e+05 1.105623e+05            234\n",
      "\n",
      "=====================================\n",
      " K·∫æT LU·∫¨N CU·ªêI C√ôNG\n",
      "=====================================\n",
      "\n",
      " Best RMSE: Strategy 1\n",
      "   ‚Üí Test RMSE: $53,335.53\n",
      "   ‚Üí MAE: $48,773.29\n",
      "\n",
      " Most Compact: Strategy 3\n",
      "   ‚Üí Features: 36\n",
      "   ‚Üí Memory: 0.31 MB\n",
      "\n",
      " Best for Deep Learning: Strategy 4 (MinMax + Embedding)\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# PART 9 ‚Äì So S√°nh C√°c Chi·∫øn L∆∞·ª£c Ti·ªÅn X·ª≠ L√Ω\n",
    "# ================================================\n",
    "\n",
    "# 1. Summary Table\n",
    "strategies_summary = []\n",
    "\n",
    "strategies_summary.append({\n",
    "    'Strategy': '1. OneHot + Standard',\n",
    "    'Train_Shape': proc1_train.shape,\n",
    "    'Test_Shape': proc1_test.shape,\n",
    "    'N_Features': proc1_train.shape[1] - 1,\n",
    "    'Description': 'One-Hot Encoding + StandardScaler'\n",
    "})\n",
    "\n",
    "if proc2_train is not None:\n",
    "    strategies_summary.append({\n",
    "        'Strategy': '2. Target + Robust',\n",
    "        'Train_Shape': proc2_train.shape,\n",
    "        'Test_Shape': proc2_test.shape,\n",
    "        'N_Features': proc2_train.shape[1] - 1,\n",
    "        'Description': 'Target Encoding + RobustScaler'\n",
    "    })\n",
    "\n",
    "strategies_summary.append({\n",
    "    'Strategy': '3. PCA + Ordinal',\n",
    "    'Train_Shape': proc3_train.shape,\n",
    "    'Test_Shape': proc3_test.shape,\n",
    "    'N_Features': proc3_train.shape[1] - 1,\n",
    "    'Description': 'PCA + Ordinal Encoding'\n",
    "})\n",
    "\n",
    "strategies_summary.append({\n",
    "    'Strategy': '4. MinMax + Embedding',\n",
    "    'Train_Shape': proc4_train.shape,\n",
    "    'Test_Shape': proc4_test.shape,\n",
    "    'N_Features': proc4_train.shape[1] - 1,\n",
    "    'Description': 'MinMaxScaler + Embedding Preparation'\n",
    "})\n",
    "\n",
    "print(\"=== STRATEGIES OVERVIEW ===\")\n",
    "print(pd.DataFrame(strategies_summary).to_string(index=False))\n",
    "\n",
    "\n",
    "# 2. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng feature\n",
    "def evaluate_strategy_quality(proc_train, strategy_name):\n",
    "    if proc_train is None:\n",
    "        return None\n",
    "    \n",
    "    features = [c for c in proc_train.columns if c != 'SalePrice']\n",
    "    X = proc_train[features]\n",
    "\n",
    "    n_missing = X.isna().sum().sum()\n",
    "    n_constant = (X.nunique() == 1).sum()\n",
    "\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    n_high_corr = (upper_tri > 0.95).sum().sum()\n",
    "\n",
    "    memory_mb = X.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    \n",
    "    return {\n",
    "        'strategy': strategy_name,\n",
    "        'n_features': len(features),\n",
    "        'n_missing': n_missing,\n",
    "        'n_constant': n_constant,\n",
    "        'n_high_corr_pairs': n_high_corr,\n",
    "        'memory_mb': memory_mb\n",
    "    }\n",
    "\n",
    "quality_results = []\n",
    "quality_results.append(evaluate_strategy_quality(proc1_train, \"Strategy 1\"))\n",
    "if proc2_train is not None:\n",
    "    quality_results.append(evaluate_strategy_quality(proc2_train, \"Strategy 2\"))\n",
    "quality_results.append(evaluate_strategy_quality(proc3_train, \"Strategy 3\"))\n",
    "quality_results.append(evaluate_strategy_quality(proc4_train, \"Strategy 4\"))\n",
    "\n",
    "quality_df = pd.DataFrame(quality_results)\n",
    "print(\"\\n=== FEATURE QUALITY ===\")\n",
    "print(quality_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# 3. Quick Ridge Regression test\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def quick_model_test(proc_train, proc_test, strategy_name):\n",
    "    if proc_train is None or proc_test is None:\n",
    "        return None\n",
    "    \n",
    "    y_train = proc_train['SalePrice']\n",
    "    y_test = proc_test['SalePrice']\n",
    "\n",
    "    train_cols = [c for c in proc_train.columns if c != 'SalePrice']\n",
    "    test_cols = [c for c in proc_test.columns if c != 'SalePrice']\n",
    "    common_cols = list(set(train_cols) & set(test_cols))\n",
    "\n",
    "    X_train = proc_train[common_cols].fillna(0)\n",
    "    X_test = proc_test[common_cols].fillna(0)\n",
    "\n",
    "    model = Ridge(alpha=1.0)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train,\n",
    "                                cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'strategy': strategy_name,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'features_used': len(common_cols)\n",
    "    }\n",
    "\n",
    "model_results = []\n",
    "model_results.append(quick_model_test(proc1_train, proc1_test, \"Strategy 1\"))\n",
    "\n",
    "if proc2_train is not None:\n",
    "    model_results.append(quick_model_test(proc2_train, proc2_test, \"Strategy 2\"))\n",
    "\n",
    "model_results.append(quick_model_test(proc3_train, proc3_test, \"Strategy 3\"))\n",
    "model_results.append(quick_model_test(proc4_train, proc4_test, \"Strategy 4\"))\n",
    "\n",
    "model_df = pd.DataFrame(model_results)\n",
    "print(\"\\n=== MODEL PERFORMANCE (Ridge) ===\")\n",
    "print(model_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# 4. K·∫øt lu·∫≠n ‚Äì chi·∫øn l∆∞·ª£c t·ªët nh·∫•t\n",
    "best_model = model_df.iloc[model_df['test_rmse'].idxmin()]\n",
    "best_quality = quality_df.iloc[quality_df['n_features'].idxmin()]\n",
    "\n",
    "print(\"\\n=====================================\")\n",
    "print(\" K·∫æT LU·∫¨N CU·ªêI C√ôNG\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "print(f\"\\n Best RMSE: {best_model['strategy']}\")\n",
    "print(f\"   ‚Üí Test RMSE: ${best_model['test_rmse']:,.2f}\")\n",
    "print(f\"   ‚Üí MAE: ${best_model['test_mae']:,.2f}\")\n",
    "\n",
    "print(f\"\\n Most Compact: {best_quality['strategy']}\")\n",
    "print(f\"   ‚Üí Features: {best_quality['n_features']}\")\n",
    "print(f\"   ‚Üí Memory: {best_quality['memory_mb']:.2f} MB\")\n",
    "\n",
    "print(f\"\\n Best for Deep Learning: Strategy 4 (MinMax + Embedding)\")\n",
    "print(\"=====================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
